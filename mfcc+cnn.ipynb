{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e821aef7-c127-4f95-98a7-d92865635a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 19:57:15.836573: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-11 19:57:15.884388: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 19:57:21.869282: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from shared import accent_mapping, dataset_path, SharedConfig\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    HubertModel, \n",
    "    HubertForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    AutoFeatureExtractor\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10b5f9b-90b4-414b-a96e-f769ef3febc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_cfg = SharedConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e7cf08-17c1-41d8-8e1e-4152e1bac68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MFCC = 40\n",
    "WIN_LENGTH = int(0.025 * shared_cfg.sample_rate)  # 25 ms window\n",
    "HOP_LENGTH = int(0.010 * shared_cfg.sample_rate)  # 10 ms stride\n",
    "\n",
    "NUM_FRAMES = 1 + (shared_cfg.max_length - WIN_LENGTH) // HOP_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03805c16-e6e9-4d16-9b21-576997e7a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForAccentClassification:\n",
    "    # def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    def __call__(self, features: List[Tuple[torch.Tensor, str]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract MFCCs and labels\n",
    "        mfccs = [f[0] for f in features]  # Each item is (mfcc, label)\n",
    "        labels = torch.tensor([f[1] for f in features], dtype=torch.long)\n",
    "\n",
    "        # Find max time length in batch\n",
    "        max_len = max(mfcc.shape[-1] for mfcc in mfccs)\n",
    "\n",
    "        # Pad all MFCCs to max_len along time dimension\n",
    "        padded_mfccs = []\n",
    "        for mfcc in mfccs:\n",
    "            if mfcc.shape[-1] < max_len:\n",
    "                pad_width = max_len - mfcc.shape[-1]\n",
    "                mfcc = F.pad(mfcc, (0, pad_width))  # pad last dim only\n",
    "            padded_mfccs.append(mfcc)\n",
    "\n",
    "        batch_mfcc = torch.stack(padded_mfccs)  # Shape: [batch, 1, 40, max_len]\n",
    "        return {\"input_values\": batch_mfcc, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19135db-cbae-4b47-8679-0145ba9aec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, csv_file, split='train', max_length=160000, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the full CSV file with file_path and accent columns.\n",
    "            split (str): One of 'train', 'val', or 'test'.\n",
    "            transform (callable, optional): MFCC transform to apply to audio.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Filter rows by split\n",
    "        self.data = self.data[self.data['split'] == split].reset_index(drop=True)\n",
    "\n",
    "        # Store file paths and labels\n",
    "        self.file_paths = [Path(fp) for fp in self.data['file_path']]\n",
    "        self.labels = [accent_mapping[label] for label in self.data['accent']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.file_paths[idx])\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sample_rate != shared_cfg.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, cfg.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Ensure waveform is 1D\n",
    "        waveform = waveform.squeeze()\n",
    "        \n",
    "        mfcc = self.transform(waveform)  # shape: [1, 40, time]\n",
    "        if mfcc.dim() == 2:  # shape: [40, T]\n",
    "            mfcc = mfcc.unsqueeze(0)  # make it [1, 40, T]\n",
    "\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return mfcc, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564cf645-1c07-4ee3-b279-cc7b69c395ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=40,\n",
    "    melkwargs={\n",
    "        'n_fft': 400,\n",
    "        'hop_length': 160,\n",
    "        'n_mels': 40\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab28b72-fa7b-4c38-84b3-a503544b0694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchaudio.transforms._transforms.MFCC"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mfcc_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5373a6b4-6ac9-4faf-8ba7-76cd9b5c8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AccentCNN(nn.Module):\n",
    "#     def __init__(self, num_classes=21):\n",
    "#         super(AccentCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "#         self.fc1 = nn.Linear(32 * 10 * 12, 128)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))  # [B, 16, H/2, W/2]\n",
    "#         x = self.pool(F.relu(self.conv2(x)))  # [B, 32, H/4, W/4]\n",
    "#         x = x.view(x.size(0), -1)             # flatten\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class AccentCNN(nn.Module):\n",
    "    def __init__(self, num_mfcc, num_classes):\n",
    "        super().__init__()\n",
    "        # super(AudioCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.AdaptiveMaxPool2d((1, 1))  # Global pooling\n",
    "\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b237e99-fbe0-4a9c-b369-62906b483fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader=None, num_epochs=10, learning_rate=0.001, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[\"input_values\"].to(device)  # shape: [B, 1, 40, T]\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            assert inputs.ndim == 4, f\"Expected input shape [B, 1, 40, T], got {inputs.shape}\"\n",
    "            assert labels.ndim == 1, f\"Expected label shape [B], got {labels.shape}\"\n",
    "            \n",
    "            outputs = model(inputs)  # shape: [B, num_classes]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / total\n",
    "        accuracy = 100. * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        if val_loader:\n",
    "            evaluate_model(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26aca04c-3516-4dc1-84bc-692418582514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch[\"input_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16eb2624-41f9-4608-9758-071134f8b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=16000,\n",
    "    n_mfcc=40,\n",
    "    melkwargs={\n",
    "        'n_fft': 400,\n",
    "        'hop_length': 160,\n",
    "        'n_mels': 40\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f258f5bb-f6b5-48c5-807f-5dc20ba0953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForAccentClassification()\n",
    "\n",
    "\n",
    "train_dataset = AccentDataset(csv_file=\"./out-min-n-20-max-t-60-augmented/accent_data.csv\", split=\"train\", transform=mfcc_transform, max_length=shared_cfg.max_length)\n",
    "val_dataset = AccentDataset(csv_file=\"./out-min-n-20-max-t-60-augmented/accent_data.csv\", split=\"val\", transform=mfcc_transform, max_length=shared_cfg.max_length)\n",
    "test_dataset = AccentDataset(csv_file=\"./out-min-n-20-max-t-60-augmented/accent_data.csv\", split=\"test\", transform=mfcc_transform, max_length=shared_cfg.max_length)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d9dab60-e94b-4959-bded-4295222f53f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 2.3867, Accuracy: 40.61%\n",
      "Epoch 2/10 - Loss: 1.6498, Accuracy: 58.98%\n",
      "Epoch 3/10 - Loss: 1.5281, Accuracy: 61.16%\n",
      "Epoch 4/10 - Loss: 1.3740, Accuracy: 62.11%\n",
      "Epoch 5/10 - Loss: 1.2971, Accuracy: 63.88%\n",
      "Epoch 6/10 - Loss: 1.2240, Accuracy: 64.49%\n",
      "Epoch 7/10 - Loss: 1.1789, Accuracy: 65.37%\n",
      "Epoch 8/10 - Loss: 1.1137, Accuracy: 68.23%\n",
      "Epoch 9/10 - Loss: 1.0668, Accuracy: 68.78%\n",
      "Epoch 10/10 - Loss: 0.9860, Accuracy: 70.14%\n"
     ]
    }
   ],
   "source": [
    "model = AccentCNN(num_mfcc=N_MFCC, num_classes=len(accent_mapping))\n",
    "train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eed4f63-3c6b-4dd9-8746-e581ae0c9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch[\"input_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Validation Accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55311dee-070e-469d-9ce8-5af4eda10845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 59.68%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12014587-844a-411c-88a8-eb8c446f778c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
