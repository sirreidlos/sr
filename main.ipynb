{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f98775-0cd0-4c24-a8bd-a466cf4487d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "836649c2-31f6-4f3e-8c9e-be545c92ffe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(16.3525)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./out-min-n-20-max-t-30-clean/accent_data.csv\")\n",
    "df[\"duration\"].min()\n",
    "# print(df[df[\"duration\"] == df[\"duration\"].max()][\"file_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03caf651-0108-49bb-aee5-eefe696d5968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/italian14.wav</td>\n",
       "      <td>italian</td>\n",
       "      <td>25.857063</td>\n",
       "      <td>dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/french36.wav</td>\n",
       "      <td>french</td>\n",
       "      <td>18.962125</td>\n",
       "      <td>dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/spanish69.wav</td>\n",
       "      <td>spanish</td>\n",
       "      <td>20.075187</td>\n",
       "      <td>dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/macedonian24.wav</td>\n",
       "      <td>macedonian</td>\n",
       "      <td>25.400125</td>\n",
       "      <td>dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/german14.wav</td>\n",
       "      <td>german</td>\n",
       "      <td>23.233750</td>\n",
       "      <td>dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                file_path  \\\n",
       "0     ./out-min-n-20-max-t-30-clean/processed_audio/dataset/italian14.wav   \n",
       "1      ./out-min-n-20-max-t-30-clean/processed_audio/dataset/french36.wav   \n",
       "2     ./out-min-n-20-max-t-30-clean/processed_audio/dataset/spanish69.wav   \n",
       "3  ./out-min-n-20-max-t-30-clean/processed_audio/dataset/macedonian24.wav   \n",
       "4      ./out-min-n-20-max-t-30-clean/processed_audio/dataset/german14.wav   \n",
       "\n",
       "       accent   duration   source  split  \n",
       "0     italian  25.857063  dataset  train  \n",
       "1      french  18.962125  dataset  train  \n",
       "2     spanish  20.075187  dataset  train  \n",
       "3  macedonian  25.400125  dataset  train  \n",
       "4      german  23.233750  dataset  train  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3655a5a3-b637-456e-aad4-a404ba11e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='file_path').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "841103dd-a25a-464e-bb51-75fa39b01ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic16.wav</td>\n",
       "      <td>arabic</td>\n",
       "      <td>19.428375</td>\n",
       "      <td>dataset</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic17.wav</td>\n",
       "      <td>arabic</td>\n",
       "      <td>21.505500</td>\n",
       "      <td>dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic2.wav</td>\n",
       "      <td>arabic</td>\n",
       "      <td>27.220875</td>\n",
       "      <td>dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic21.wav</td>\n",
       "      <td>arabic</td>\n",
       "      <td>26.906063</td>\n",
       "      <td>dataset</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic32.wav</td>\n",
       "      <td>arabic</td>\n",
       "      <td>26.891125</td>\n",
       "      <td>dataset</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            file_path  accent  \\\n",
       "0  ./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic16.wav  arabic   \n",
       "1  ./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic17.wav  arabic   \n",
       "2   ./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic2.wav  arabic   \n",
       "3  ./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic21.wav  arabic   \n",
       "4  ./out-min-n-20-max-t-30-clean/processed_audio/dataset/arabic32.wav  arabic   \n",
       "\n",
       "    duration   source  split  \n",
       "0  19.428375  dataset    val  \n",
       "1  21.505500  dataset  train  \n",
       "2  27.220875  dataset  train  \n",
       "3  26.906063  dataset  train  \n",
       "4  26.891125  dataset   test  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec1a599-aea4-4fe9-a4d8-e051e360aea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of items with duration below 30: 100.0%\n"
     ]
    }
   ],
   "source": [
    "percentage = (df[df[\"duration\"] < 30].shape[0] / df.shape[0]) * 100\n",
    "print(f\"Percentage of items with duration below 30: {percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74e8e45c-2d3e-40db-8169-c757856ad3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'korean': 0,\n",
       " 'arabic': 1,\n",
       " 'french': 2,\n",
       " 'english': 3,\n",
       " 'mandarin': 4,\n",
       " 'portuguese': 5,\n",
       " 'turkish': 6,\n",
       " 'russian': 7,\n",
       " 'italian': 8,\n",
       " 'dutch': 9,\n",
       " 'polish': 10,\n",
       " 'german': 11,\n",
       " 'swedish': 12,\n",
       " 'macedonian': 13,\n",
       " 'spanish': 14}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accents = {k:v for (v, k) in enumerate(df[\"accent\"].unique())}\n",
    "accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bda39c5-e5bc-41af-ada5-c4c2c20cb210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n",
      "('self', 'output_dir', 'overwrite_output_dir', 'do_train', 'do_eval', 'do_predict', 'eval_strategy', 'prediction_loss_only', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'per_gpu_train_batch_size', 'per_gpu_eval_batch_size', 'gradient_accumulation_steps', 'eval_accumulation_steps', 'eval_delay', 'torch_empty_cache_steps', 'learning_rate', 'weight_decay', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'max_grad_norm', 'num_train_epochs', 'max_steps', 'lr_scheduler_type', 'lr_scheduler_kwargs', 'warmup_ratio', 'warmup_steps', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_strategy', 'logging_first_step', 'logging_steps', 'logging_nan_inf_filter', 'save_strategy', 'save_steps', 'save_total_limit', 'save_safetensors', 'save_on_each_node', 'save_only_model', 'restore_callback_states_from_checkpoint', 'no_cuda', 'use_cpu', 'use_mps_device', 'seed', 'data_seed', 'jit_mode_eval', 'use_ipex', 'bf16', 'fp16', 'fp16_opt_level', 'half_precision_backend', 'bf16_full_eval', 'fp16_full_eval', 'tf32', 'local_rank', 'ddp_backend', 'tpu_num_cores', 'tpu_metrics_debug', 'debug', 'dataloader_drop_last', 'eval_steps', 'dataloader_num_workers', 'dataloader_prefetch_factor', 'past_index', 'run_name', 'disable_tqdm', 'remove_unused_columns', 'label_names', 'load_best_model_at_end', 'metric_for_best_model', 'greater_is_better', 'ignore_data_skip', 'fsdp', 'fsdp_min_num_params', 'fsdp_config', 'tp_size', 'fsdp_transformer_layer_cls_to_wrap', 'accelerator_config', 'deepspeed', 'label_smoothing_factor', 'optim', 'optim_args', 'adafactor', 'group_by_length', 'length_column_name', 'report_to', 'ddp_find_unused_parameters', 'ddp_bucket_cap_mb', 'ddp_broadcast_buffers', 'dataloader_pin_memory', 'dataloader_persistent_workers', 'skip_memory_metrics', 'use_legacy_prediction_loop', 'push_to_hub', 'resume_from_checkpoint', 'hub_model_id', 'hub_strategy', 'hub_token', 'hub_private_repo', 'hub_always_push', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'include_inputs_for_metrics', 'include_for_metrics', 'eval_do_concat_batches', 'fp16_backend', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', 'mp_parameters', 'auto_find_batch_size', 'full_determinism', 'torchdynamo', 'ray_scope', 'ddp_timeout', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'include_tokens_per_second', 'include_num_input_tokens_seen', 'neftune_noise_alpha', 'optim_target_modules', 'batch_eval_metrics', 'eval_on_start', 'use_liger_kernel', 'eval_use_gather_object', 'average_tokens_across_devices')\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.TrainingArguments.__init__.__code__.co_varnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ca8bb4-f381-448e-a197-95daefa98712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 12:44:21.064261: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-08 12:44:21.454469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 12:44:29.710521: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, HubertModel\n",
    "import soundfile as sf\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8dd02-338b-45de-97cb-052f3cc94b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import HubertModel, HubertConfig, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Sample dataset class (replace with your own data loading)\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, feature_extractor):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio waveform (e.g., using torchaudio)\n",
    "        waveform, sr = torchaudio.load(self.audio_paths[idx])\n",
    "        # Resample if needed, ensure 16kHz\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "        # feature_extractor returns dict with 'input_values' key\n",
    "        inputs = self.feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "        labels = torch.tensor(self.labels[idx])\n",
    "        return inputs.input_values.squeeze(0), labels\n",
    "\n",
    "# Model wrapper for accent classification\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class HubertForAccentClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"facebook/hubert-base-ls960\", num_labels=10, freeze_feature_extractor=True):\n",
    "        super().__init__()\n",
    "        # Load pretrained HuBERT\n",
    "        self.hubert = HubertModel.from_pretrained(pretrained_model_name)\n",
    "        if freeze_feature_extractor:\n",
    "            # Freeze the CNN (feature extractor)\n",
    "            for param in self.hubert.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Encoder output dimension\n",
    "        hidden_size = self.hubert.config.hidden_size\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None, labels=None):\n",
    "        # input_values: (batch, seq_len)\n",
    "        outputs = self.hubert(input_values, attention_mask=attention_mask)\n",
    "        # Take mean pooling over time\n",
    "        hidden_states = outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "        pooled = hidden_states.mean(dim=1)        # (batch, hidden_size)\n",
    "        logits = self.classifier(pooled)          # (batch, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for input_values, labels in dataloader:\n",
    "        input_values = input_values.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_values, labels=labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torchaudio\n",
    "    # Replace with your data\n",
    "    train_audio_paths = [\"path/to/audio1.wav\", \"path/to/audio2.wav\", ...]\n",
    "    train_labels = [0, 1, ...]  # numeric accent labels\n",
    "\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, return_attention_mask=False)\n",
    "    train_dataset = AccentDataset(train_audio_paths, train_labels, feature_extractor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = HubertForAccentClassification(num_labels=len(set(train_labels))).to(device)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "    epochs = 5\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = train(model, train_loader, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(\"./hubert-accent-model\")\n",
    "    print(\"Model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
